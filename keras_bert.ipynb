{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/terence-bigtt/bert/blob/master/keras_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "uA4moo4dpLBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# keras-bert prediction with Cloud TPU\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\" >\n",
        " <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/HighCWu/keras-bert-tpu/blob/master/demo/load_model/load_and_predict.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "Uk6wtvQsP-3R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Install Dependences\n",
        "! pip install keras-bert-tpu -q\n",
        "! pip install bert-tensorflow\n",
        "! pip install tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GBFMypMHSHlt",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# @title Download Pretrained Podel\n",
        "import os\n",
        "UPLOAD_TIME = '2018_11_03' #@param {type:\"string\"}\n",
        "BERT_MODEL = 'multilingual_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "download_url = 'https://storage.googleapis.com/bert_models/{}/{}.zip'.format(UPLOAD_TIME,BERT_MODEL)\n",
        "zip_path = '{}.zip'.format(BERT_MODEL)\n",
        "! test -d $BERT_MODEL || (wget $download_url && unzip $zip_path)\n",
        "BERT_PRETRAINED_DIR = os.path.realpath(BERT_MODEL)\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J6uDbcJUxxmL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import bert\n",
        "from bert import tokenization\n",
        "tokenizer = tokenization.FullTokenizer(os.path.join(BERT_PRETRAINED_DIR, \"vocab.txt\"))\n",
        "#tokenizer.tokenize(\"Bonjour, ceci est un test, est-ce que les noms propres comme Térence Delsate ou Mickaël Tits sont connus ? ou alors parlons de Daffalgan ou encore de rhumatisme pyramidaloïde, même si ça n'existe pas ?\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VBuevVi2ykfA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import codecs\n",
        "import numpy as np\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "model_name = \"uncased_L-12_H-768_A-12\"\n",
        "\n",
        "#C:/Users/mti/Documents/python/bert\n",
        "\n",
        "config_path = BERT_PRETRAINED_DIR + \"/bert_config.json\"\n",
        "checkpoint_path = BERT_PRETRAINED_DIR + \"/bert_model.ckpt\"\n",
        "dict_path = BERT_PRETRAINED_DIR + \"/vocab.txt\"\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(dict_path)    \n",
        "\n",
        "with tf.device('/gpu:0'):  \n",
        "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n",
        "#    model.summary(line_length=120)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "\n",
        "#tokens = tokenizer.tokenize(\"Bonjour, ceci est un test, est-ce que les noms propres comme Térence Delsate ou Mickaël Tits sont connus ? ou alors parlons de Daffalgan ou encore de rhumatisme pyramidaloïde, même si ça n'existe pas ?\")\n",
        "#tokens = tokenizer.tokenize(\"Hello, this is a test, are proper names like Terence Delsate or Michael Tits known? Or do we talk about Daffalgan or even pyramidal rheumatism, even if it does not exist?\")    \n",
        "\n",
        "with open(dict_path, 'r') as f: \n",
        "  token_dict = {t.strip(): i for i, t in enumerate(f.readlines())} \n",
        "\n",
        "def mean(lis):\n",
        "  return sum(lis)/len(lis)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_eL50uLdevyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "  \n",
        "messages = [\"Clothilde fête son anniversaire avec ses amies de maternelle.\",\n",
        "            \"Une petite fille nommée Clothilde a invité ses copines pour son anniversaire\",\n",
        "            \"L'ingénieurie est le petit frère attardé de la physique\"]\n",
        "\n",
        "#tokenize all sentences\n",
        "messagetokens = [ ['[CLS]']+tokenizer.tokenize(message)+['[SEP]'] for message in messages]\n",
        "\n",
        "token_inputs = [None]*len(messagetokens)\n",
        "seg_inputs = [None]*len(messagetokens)\n",
        "\n",
        "\n",
        "#tokens to features (token ids in voab.txt)\n",
        "for tokens, i in zip(messagetokens,range(0,len(messagetokens))):\n",
        "    token_inputs[i] =  np.asarray([token_dict[token] for token in tokens] + [0] * (512 - len(tokens)))[0:512]\n",
        "    seg_inputs[i] = np.asarray([0] *512 )[0:512]\n",
        "    \n",
        "with tf.device('/gpu:0'):    \n",
        "    predictions = model.predict([token_inputs, seg_inputs])\n",
        "\n",
        "#trim predictions to real token size\n",
        "\n",
        "predictions = [prediction[0:len(tokens)] for prediction, tokens in zip(predictions,messagetokens) ]\n",
        "\n",
        "s1= mean(list(predictions[0]))\n",
        "s2= mean(list(predictions[1]))\n",
        "s3= mean(list(predictions[2]))\n",
        "  \n",
        "print(s1.dot(s2)/np.sqrt(s1.dot(s1) * s2.dot(s2)))\n",
        "print(s1.dot(s3)/np.sqrt(s1.dot(s1) * s3.dot(s3)))\n",
        "print(s2.dot(s3)/np.sqrt(s2.dot(s2) * s3.dot(s3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0tCOTjN7fcxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oPr6k7qihK1p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GZo1wqEKgtrw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiaEW5laTN6z",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0jgbOvQ3sMkW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras_bert. import tok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHn1rWFLuubx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_tpu=True # @param {type:\"boolean\"}\n",
        "if use_tpu:\n",
        "  assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; Maybe you should switch hardware accelerator to TPU for TPU support'\n",
        "  import tensorflow as tf\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  strategy = tf.contrib.tpu.TPUDistributionStrategy(\n",
        "          tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_address)\n",
        "  )\n",
        "  model = tf.contrib.tpu.keras_to_tpu_model(\n",
        "                      model, strategy=strategy)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Vrs_AqDe_RH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import codecs\n",
        "import numpy as np\n",
        "\n",
        "bsz = 8 # TPU batch size must be a mutiple of 8\n",
        "\n",
        "dict_path = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "\n",
        "tokens = ['[CLS]', '[MASK]', '[MASK]'] + list('是利用符号语言研究数量、结构、变化以及空间等概念的一门学科') + ['[SEP]']\n",
        "\n",
        "token_dict = {}\n",
        "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        token_dict[token] = len(token_dict)\n",
        "token_dict_rev = {v: k for k, v in token_dict.items()}\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * len(tokens) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0, 1, 1] + [0] * (512 - 3) for i in range(bsz)])\n",
        "\n",
        "\n",
        "print(token_input[0][:len(tokens)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "print(predicts[0][:len(tokens)])\n",
        "print(list(map(lambda x: token_dict_rev[x], predicts[0][1:3])))\n",
        "\n",
        "\n",
        "sentence_1 = '数学是利用符号语言研究數量、结构、变化以及空间等概念的一門学科。'\n",
        "sentence_2 = '从某种角度看屬於形式科學的一種。'\n",
        "\n",
        "tokens = ['[CLS]'] + list(sentence_1) + ['[SEP]'] + list(sentence_2) + ['[SEP]']\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * (len(sentence_1) + 2) + [1] * (len(sentence_2) + 1) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0] * 512 for i in range(bsz)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]\n",
        "print('%s is random next: ' % sentence_2, bool(np.argmax(predicts, axis=-1)[0]))\n",
        "\n",
        "sentence_2 = '任何一个希尔伯特空间都有一族标准正交基。'\n",
        "\n",
        "tokens = ['[CLS]'] + list(sentence_1) + ['[SEP]'] + list(sentence_2) + ['[SEP]']\n",
        "\n",
        "token_input = np.asarray([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "seg_input = np.asarray([[0] * (len(sentence_1) + 2) + [1] * (len(sentence_2) + 1) + [0] * (512 - len(tokens)) for i in range(bsz)])\n",
        "mask_input = np.asarray([[0] * 512 for i in range(bsz)])\n",
        "\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]\n",
        "print('%s is random next: ' % sentence_2, bool(np.argmax(predicts, axis=-1)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}